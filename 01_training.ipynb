{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Sentiment Analysis Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn==1.1.1\n",
      "pandas==1.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep scikit-learn\n",
    "!pip freeze | grep pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bengsoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/bengsoon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/bengsoon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/bengsoon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/bengsoon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#TODO:\n",
    "######### these should be set upfront at the environment level (eg docker level) #########\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "##########################################################################################\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'data': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(data_path / \"training.csv\")\n",
    "df_valid = pd.read_csv(data_path / \"validation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in [`00_preparation.ipynb`](\"./00_preparation.ipynb\"), our `df_train` is quite well-balanced. So we don't have to quite worry about [imbalanced dataset](\"https://github.com/bengsoon/Handling_Imbalanced_Data\") in this project. This is great news to us as we do not want to digress from our main objective âž¡ **ACTIVE LEARNING** ðŸ™ŒðŸ¥³ðŸŽ‰ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    2511\n",
       "positive    2489\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    5040\n",
       "positive    4960\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be employing a simple Naive Bayes classification model using the TF-idf method. As such, our preprocessing pipeline will cater to that\n",
    "\n",
    "A quick research on the Internet provided us some quick insights to the dataset: \n",
    "1. The review texts contain HTML tags, so we should remove those.\n",
    "2. There are brackets that should be removed.\n",
    "3. There are also non-alphanumeric characters that should be removed.\n",
    "\n",
    "We will use `regex` / `re` to clean up our dataset.\n",
    "\n",
    "References: https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str: \n",
    "    '''\n",
    "     - removes all html tags\n",
    "     - replaces all whitespaces and non-alphanumeric characters with ' '\n",
    "     - returns lowercase\n",
    "    '''\n",
    "\n",
    "    # remove html tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # replace non-alphanumeric\n",
    "    text = re.sub(\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "\n",
    "    # replace unnecessary whitespaces\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "<br /><br />\"\"The Grudge 2\"\" has scary sound and visual effects, with the creepy woman and boy, and I have startled a couple of times while watching this movie.      However, the complex screenplay with three subplots is totally confused, making the entwined story a complete mess. There are too much characters and situations,     and in a certain moment I was completely lost with the disconnected and fragmented narrative. In the end, I was completely disappointed with this confused, but also spooky film.     My vote is four.<br /><br />\n",
      "\n",
      "Cleaned String:\n",
      " the grudge 2 has scary sound and visual effects with the creepy woman and boy and i have startled a couple of times while watching this movie however the complex screenplay with three subplots is totally confused making the entwined story a complete mess there are too much characters and situations and in a certain moment i was completely lost with the disconnected and fragmented narrative in the end i was completely disappointed with this confused but also spooky film my vote is four \n"
     ]
    }
   ],
   "source": [
    "# test out function\n",
    "test_string = '''<br /><br />\"\"The Grudge 2\"\" has scary sound and visual effects, with the creepy woman and boy, and I have startled a couple of times while watching this movie. \\\n",
    "     However, the complex screenplay with three subplots is totally confused, making the entwined story a complete mess. There are too much characters and situations, \\\n",
    "    and in a certain moment I was completely lost with the disconnected and fragmented narrative. In the end, I was completely disappointed with this confused, but also spooky film. \\\n",
    "    My vote is four.<br /><br />'''\n",
    "\n",
    "cleaned_text = clean_text(test_string)\n",
    "print(f\"Original Text:\\n{test_string}\\n\")\n",
    "print(f\"Cleaned String:\\n{cleaned_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "We will remove the commonly occuring words in the English using NLTK's stopwords dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str) -> str:\n",
    "    # removes common English stopwords with nltk\n",
    "\n",
    "    stopwords_dict = set(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords_dict]\n",
    "    \n",
    "    return ' '.join(filtered_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text:\n",
      " the grudge 2 has scary sound and visual effects with the creepy woman and boy and i have startled a couple of times while watching this movie however the complex screenplay with three subplots is totally confused making the entwined story a complete mess there are too much characters and situations and in a certain moment i was completely lost with the disconnected and fragmented narrative in the end i was completely disappointed with this confused but also spooky film my vote is four \n",
      "\n",
      "Removed Stopwords:\n",
      "grudge 2 scary sound visual effects creepy woman boy startled couple times watching movie however complex screenplay three subplots totally confused making entwined story complete mess much characters situations certain moment completely lost disconnected fragmented narrative end completely disappointed confused also spooky film vote four\n"
     ]
    }
   ],
   "source": [
    "# test remove_stopwords\n",
    "cleaned_stopwords_text = remove_stopwords(cleaned_text)\n",
    "print(f\"Cleaned Text:\\n{cleaned_text}\\n\")\n",
    "print(f\"Removed Stopwords:\\n{cleaned_stopwords_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization: Lemmatization or Stemming\n",
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Stopwords Text:\n",
      "grudge 2 scary sound visual effects creepy woman boy startled couple times watching movie however complex screenplay three subplots totally confused making entwined story complete mess much characters situations certain moment completely lost disconnected fragmented narrative end completely disappointed confused also spooky film vote four\n",
      "\n",
      "Lemmatized Text:\n",
      "grudge 2 scary sound visual effect creepy woman boy startle couple time watch movie however complex screenplay three subplots totally confuse make entwine story complete mess much character situation certain moment completely lose disconnect fragment narrative end completely disappointed confuse also spooky film vote four\n"
     ]
    }
   ],
   "source": [
    "def pos_tagger(word):\n",
    "    \"\"\"\n",
    "    Obtains the Parts of Speech (POS) for NLTK's lemmatizer mapping\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"j\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "\n",
    "    # returns the pos tag, defaults to noun\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_text = ' '.join([lemmatizer.lemmatize(w, pos_tagger(w)) for w in word_tokenize(cleaned_stopwords_text)])\n",
    "print(f\"Cleaned Stopwords Text:\\n{cleaned_stopwords_text}\\n\")\n",
    "print(f\"Lemmatized Text:\\n{lemmatized_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grudg 2 scari sound visual effect creepi woman boy startl coupl time watch movi howev complex screenplay three subplot total confus make entwin stori complet mess much charact situat certain moment complet lost disconnect fragment narr end complet disappoint confus also spooki film vote four'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "' '.join([stemmer.stem(word) for word in cleaned_stopwords_text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Bringing it altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {}\n",
    "CONFIG[\"NORMALIZER\"] = \"stemmer\"\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\" \n",
    "    Args\n",
    "        - text: text to be preprocessed\n",
    "        - normalizer: \n",
    "            - normalization method (\"stemmer\" or \"lemmatizer\").\n",
    "            - stemmer is simplistic that chops off the end of the words while \n",
    "                lemmatizer would bring in vocabulary and morphological analysis\n",
    "                and aiming to return the base form of the word (lemma)\n",
    "            - stemmer is computionally cheaper and simpler at the expense of inaccuracy\n",
    "    \"\"\" \n",
    "    normalizer = CONFIG[\"NORMALIZER\"]\n",
    "    \n",
    "    # remove html tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # replace non-alphanumeric\n",
    "    text = re.sub(\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "\n",
    "    # replace unnecessary whitespaces\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "\n",
    "    # removes common English stopwords with nltk\n",
    "    stopwords_dict = set(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords_dict]\n",
    "    \n",
    "    # normalize\n",
    "    \n",
    "    if normalizer == \"stemmer\": \n",
    "        stemmer = PorterStemmer()\n",
    "        text = ' '.join([stemmer.stem(word) for word in filtered_tokens])\n",
    "    \n",
    "    elif normalizer == \"lemmatizer\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = ' '.join([lemmatizer.lemmatize(w, pos_tagger(w)) for w in filtered_tokens])\n",
    "    else:\n",
    "        raise Exception(\"please enter normalizer as 'stemmer' or 'lemmatizer'\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\"House of Games is spell binding. It's so nice to occasionally see films that are perfect tens.     There are few movies I've seen that can grip you so quickly. From the opening scene this movie just gets you.<br /><br />    I'm trying really hard not to give to much away to those who may not yet have seen this but there will be a FEW SPOILERS SO DON'T READ ANYMORE IF YOU DON'T WANT TO KNOW.<br /><br />    I would say House of Games is not just a superb film but is the best movie about con artists I have ever seen-bar none.    From the moment the movie is over it begs to be replayed.<br /><br />Lindsay Crouse as Margaret Ford is simply perfection, from her mannerisms to the inflection of her voice    she gets into the role immediately. Joe Mantegna was also wonderful. The dialogue in this movie has an unforced almost unscripted quality    and these two people communicate as much in a look as they do with their voices. I also loved the way the movie was filmed, in that grainy,    surreal type of way, it fit perfectly and helped make the film what it was.<br /><br />There were a few movies I've seen and loved that this reminded me of including    The Grifters and The usual Suspects but really, House of games is completely different in it's way. Margaret and Mike are two of the most absorbing characters    I've seen on the big screen and not only do they have screen chemistry that is strong and palpable from the moment they meet,    but the buildup that starts from the moment they set eyes on each other is electrifying. You know something's going to happen but you have no idea what.    And just when you think you've guessed what the \"\"something\"\" is, you realize you haven't even scratched the surface....<br /><br />\n",
      "\n",
      "Preprocessed Text with Stemmer:\n",
      "hous game spell bind nice occasion see film perfect ten movi seen grip quickli open scene movi get tri realli hard give much away may yet seen spoiler read anymor want know would say hous game superb film best movi con artist ever seen bar none moment movi beg replay lindsay crous margaret ford simpli perfect manner inflect voic get role immedi joe mantegna also wonder dialogu movi unforc almost unscript qualiti two peopl commun much look voic also love way movi film graini surreal type way fit perfectli help make film movi seen love remind includ grifter usual suspect realli hous game complet differ way margaret mike two absorb charact seen big screen screen chemistri strong palpabl moment meet buildup start moment set eye electrifi know someth go happen idea think guess someth realiz even scratch surfac\n",
      "\n",
      "Preprocessed Text with Lemmatizer:\n",
      "House Games spell binding nice occasionally see film perfect ten movie see grip quickly opening scene movie get try really hard give much away may yet see SPOILERS READ ANYMORE WANT KNOW would say House Games superb film best movie con artist ever see bar none moment movie begs replayed Lindsay Crouse Margaret Ford simply perfection mannerism inflection voice get role immediately Joe Mantegna also wonderful dialogue movie unforced almost unscripted quality two people communicate much look voice also love way movie film grainy surreal type way fit perfectly help make film movie see love remind include Grifters usual Suspects really House game completely different way Margaret Mike two absorb character see big screen screen chemistry strong palpable moment meet buildup start moment set eye electrify know something go happen idea think guess something realize even scratch surface\n"
     ]
    }
   ],
   "source": [
    "# test out function\n",
    "test_string = '''\"House of Games is spell binding. It's so nice to occasionally see films that are perfect tens.\\\n",
    "     There are few movies I've seen that can grip you so quickly. From the opening scene this movie just gets you.<br /><br />\\\n",
    "    I'm trying really hard not to give to much away to those who may not yet have seen this but there will be a FEW SPOILERS SO DON'T READ ANYMORE IF YOU DON'T WANT TO KNOW.<br /><br />\\\n",
    "    I would say House of Games is not just a superb film but is the best movie about con artists I have ever seen-bar none.\\\n",
    "    From the moment the movie is over it begs to be replayed.<br /><br />Lindsay Crouse as Margaret Ford is simply perfection, from her mannerisms to the inflection of her voice\\\n",
    "    she gets into the role immediately. Joe Mantegna was also wonderful. The dialogue in this movie has an unforced almost unscripted quality\\\n",
    "    and these two people communicate as much in a look as they do with their voices. I also loved the way the movie was filmed, in that grainy,\\\n",
    "    surreal type of way, it fit perfectly and helped make the film what it was.<br /><br />There were a few movies I've seen and loved that this reminded me of including\\\n",
    "    The Grifters and The usual Suspects but really, House of games is completely different in it's way. Margaret and Mike are two of the most absorbing characters\\\n",
    "    I've seen on the big screen and not only do they have screen chemistry that is strong and palpable from the moment they meet,\\\n",
    "    but the buildup that starts from the moment they set eyes on each other is electrifying. You know something's going to happen but you have no idea what.\\\n",
    "    And just when you think you've guessed what the \"\"something\"\" is, you realize you haven't even scratched the surface....<br /><br />'''\n",
    "\n",
    "print(f\"Original Text:\\n{test_string}\\n\")\n",
    "print(f\"Preprocessed Text with Stemmer:\\n{preprocess_text(test_string)}\\n\")\n",
    "print(f\"Preprocessed Text with Lemmatizer:\\n{preprocess_text(test_string, 'lemmatizer')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG[\"NORMALIZER\"] = \"stem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.2 s Â± 29 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "\n",
    "list(map(preprocess_text, (df_train[\"review\"].to_list())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG[\"NORMALIZER\"] = \"lemma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 16s Â± 2.62 s per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "\n",
    "list(map(preprocess_text, (df_train[\"review\"].to_list())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text: str) -> str:\n",
    "    # remove html tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # replace non-alphanumeric\n",
    "    text = re.sub(\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "\n",
    "    # replace unnecessary whitespaces\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "\n",
    "    normalizer = CONFIG[\"NORMALIZER\"]\n",
    "\n",
    "    if normalizer == \"stemmer\": \n",
    "        stemmer = PorterStemmer()\n",
    "        text = ' '.join([stemmer.stem(word) for word in filtered_tokens])\n",
    "    \n",
    "    elif normalizer == \"lemmatizer\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = ' '.join([lemmatizer.lemmatize(w, pos_tagger(w)) for w in filtered_tokens])\n",
    "    else:\n",
    "        raise Exception('Please enter CONFIG[\"NORMALIZER\"] as \"stemmer\" or \"lemmatizer\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text: str) -> str:\n",
    "    '''\n",
    "    Preprocessor for the input features\n",
    "    '''\n",
    "\n",
    "    # remove html tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # replace non-alphanumeric\n",
    "    text = re.sub(\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "\n",
    "    # replace unnecessary whitespaces\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "\n",
    "    normalizer = CONFIG[\"NORMALIZER\"]\n",
    "\n",
    "    if normalizer == \"stem\": \n",
    "        stemmer = PorterStemmer()\n",
    "        text = ' '.join([stemmer.stem(word) for word in word_tokenize(text)])\n",
    "    \n",
    "    elif normalizer == \"lemma\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = ' '.join([lemmatizer.lemmatize(w, pos_tagger(w)) for w in word_tokenize(text)])\n",
    "    else:\n",
    "        raise Exception('Please enter CONFIG[\"NORMALIZER\"] as \"stem\" or \"lemma\"')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vectorizer for the input features\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    preprocessor=preprocessor,\n",
    "    analyzer='word',\n",
    "    stop_words=stopwords.words(\"english\"),\n",
    "    ngram_range = (1,3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bengsoon/anaconda3/envs/IMDB/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# using stemming as normalization\n",
    "CONFIG[\"NORMALIZER\"] = \"stem\"\n",
    "\n",
    "# transform training data and get labels\n",
    "X_train = tfidf_vectorizer.fit_transform(df_train[\"review\"].values)\n",
    "y_train = df_train[\"sentiment\"].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform validation data\n",
    "X_valid = tfidf_vectorizer.transform(df_valid[\"review\"].values)\n",
    "y_valid = df_valid[\"sentiment\"].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"First of all, let's get a few things straight here: a) I AM an anime fan- always has been as a matter of fact (I used to watch Speed Racer all the time in Preschool). b) I DO like several B-Movies because they're hilarious. c) I like the Godzilla movies- a lot.<br /><br />Moving on, when the movie first comes on, it seems like it's going to be your usual B-movie, down to the crappy FX, but all a sudden- BOOM! the anime comes on! This is when the movie goes WWWAAAAAYYYYY downhill.<br /><br />The animation is VERY bad & cheap, even worse than what I remember from SPEED RACER, for crissakes! In fact, it's so cheap, one of the few scenes from the movie I \"\"vividly\"\" remember is when a bunch of kids run out of a school... & it's the same kids over & over again! The FX are terrible, too; the dinosaurs look worse than Godzilla. In addition, the transition to live action to animation is unorganized, the dialogue & voices(especially the English dub that I viewed) was horrid & I was begging my dad to take the tape out of the DVD/ VHS player; The only thing that kept me surviving was cracking out jokes & comments like the robots & Joel/Mike on MST3K (you pick the season). Honestly, this is the only way to barely enjoy this movie & survive it at the same time.<br /><br />Heck, I'm planning to show this to another fellow otaku pal of mine on Halloween for a B-Movie night. Because it's stupid, pretty painful to watch & unintentionally hilarious at the same time, I'm giving this movie a 3/10, an improvement from the 0.5/10 I was originally going to give it.<br /><br />(According to my grading scale: 3/10 means Pretty much both boring & bad. As fun as counting to three unless you find a way to make fun of it, then it will become as fun as counting to 15.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative'], dtype='<U8')"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.predict(tfidf_vectorizer.transform([test_sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8566"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, nb_clf.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('IMDB')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af7a63553deff712e3f8fa7d2d5d7d35240d4ed129a65473b70a9331f46b6c93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
